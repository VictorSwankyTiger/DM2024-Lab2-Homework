{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87232,"databundleVersionId":9912598,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T17:40:20.437546Z","iopub.execute_input":"2024-12-02T17:40:20.437970Z","iopub.status.idle":"2024-12-02T17:40:24.355561Z","shell.execute_reply.started":"2024-12-02T17:40:20.437935Z","shell.execute_reply":"2024-12-02T17:40:24.354596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T17:40:50.217562Z","iopub.execute_input":"2024-12-02T17:40:50.218167Z","iopub.status.idle":"2024-12-02T17:40:51.697194Z","shell.execute_reply.started":"2024-12-02T17:40:50.218123Z","shell.execute_reply":"2024-12-02T17:40:51.695772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = '/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv'  \nidentification = pd.read_csv(file_path)\n\nfile_path = '/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv'  \nemotion = pd.read_csv(file_path)\n\nfile_path = '/kaggle/input/dm-2024-isa-5810-lab-2-homework/sampleSubmission.csv'  \nsample = pd.read_csv(file_path)\n\nfile_path = '/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json' \ntweets = pd.read_json(file_path, lines=True)\n\ntweets.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:21:57.205122Z","iopub.execute_input":"2024-12-02T18:21:57.205532Z","iopub.status.idle":"2024-12-02T18:22:37.097284Z","shell.execute_reply.started":"2024-12-02T18:21:57.205498Z","shell.execute_reply":"2024-12-02T18:22:37.095879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract relevant fields\ntweets['text'] = tweets['_source'].apply(lambda x: x['tweet']['text'])\ntweets['hashtags'] = tweets['_source'].apply(lambda x: x['tweet']['hashtags'])\ntweets['tweet_id'] = tweets['_source'].apply(lambda x: x['tweet']['tweet_id'])\nprint(tweets['text'])\nprint(tweets['hashtags'])\nprint(tweets['tweet_id'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:23:01.910320Z","iopub.execute_input":"2024-12-02T18:23:01.910833Z","iopub.status.idle":"2024-12-02T18:23:05.132623Z","shell.execute_reply.started":"2024-12-02T18:23:01.910697Z","shell.execute_reply":"2024-12-02T18:23:05.131435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#data set is to large, sample first and then produce preprocess\ntweets = tweets.merge(emotion, on='tweet_id', how='left', suffixes=(None, '_dup'))\n# Merge tweets with identifier to get train and test splits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:23:20.510734Z","iopub.execute_input":"2024-12-02T18:23:20.511172Z","iopub.status.idle":"2024-12-02T18:23:26.276743Z","shell.execute_reply.started":"2024-12-02T18:23:20.511136Z","shell.execute_reply":"2024-12-02T18:23:26.274186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tweets.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:23:26.278769Z","iopub.execute_input":"2024-12-02T18:23:26.279203Z","iopub.status.idle":"2024-12-02T18:23:26.303587Z","shell.execute_reply.started":"2024-12-02T18:23:26.279164Z","shell.execute_reply":"2024-12-02T18:23:26.301852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tweets = tweets.merge(identification)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:23:29.136807Z","iopub.execute_input":"2024-12-02T18:23:29.137200Z","iopub.status.idle":"2024-12-02T18:23:35.301781Z","shell.execute_reply.started":"2024-12-02T18:23:29.137164Z","shell.execute_reply":"2024-12-02T18:23:35.300326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tweets.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:23:53.851235Z","iopub.execute_input":"2024-12-02T18:23:53.851715Z","iopub.status.idle":"2024-12-02T18:23:53.872807Z","shell.execute_reply.started":"2024-12-02T18:23:53.851652Z","shell.execute_reply":"2024-12-02T18:23:53.871420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into train and test tweets\ntrain_tweet = tweets[tweets['identification'] == 'train']\ntest_tweet = tweets[tweets['identification'] == 'test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:24:15.934875Z","iopub.execute_input":"2024-12-02T18:24:15.935291Z","iopub.status.idle":"2024-12-02T18:24:17.102964Z","shell.execute_reply.started":"2024-12-02T18:24:15.935255Z","shell.execute_reply":"2024-12-02T18:24:17.101895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tweet = train_tweet.sample(frac=0.3, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:24:28.569159Z","iopub.execute_input":"2024-12-02T18:24:28.569708Z","iopub.status.idle":"2024-12-02T18:24:30.074742Z","shell.execute_reply.started":"2024-12-02T18:24:28.569642Z","shell.execute_reply":"2024-12-02T18:24:30.073490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')  # Ensure proper lemmatization support","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T17:43:04.780356Z","iopub.execute_input":"2024-12-02T17:43:04.780829Z","iopub.status.idle":"2024-12-02T17:43:05.085290Z","shell.execute_reply.started":"2024-12-02T17:43:04.780788Z","shell.execute_reply":"2024-12-02T17:43:05.083951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocessing function with added checks\ndef preprocess_text(text):\n    # Handle missing or non-string entries\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove URLs (http://, https://, www)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    \n    # Remove mentions (@username) and hashtags (#hashtag)\n    text = re.sub(r'@\\w+|#\\w+', '', text)\n    \n    # Remove special characters, punctuation, and numbers\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Lemmatize tokens\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    # Rejoin tokens back into a single string\n    return ' '.join(tokens)\n\n# Apply preprocessing to the 'text' column\ntrain_tweet['cleaned_text'] = train_tweet['text'].apply(preprocess_text)\n\n# Check the results\nprint(tweets[['text', 'cleaned_text']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T17:43:12.361354Z","iopub.execute_input":"2024-12-02T17:43:12.361768Z","iopub.status.idle":"2024-12-02T17:46:22.088097Z","shell.execute_reply.started":"2024-12-02T17:43:12.361729Z","shell.execute_reply":"2024-12-02T17:46:22.086723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tweet['cleaned_text'].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T17:48:48.361892Z","iopub.execute_input":"2024-12-02T17:48:48.362287Z","iopub.status.idle":"2024-12-02T17:48:48.371902Z","shell.execute_reply.started":"2024-12-02T17:48:48.362256Z","shell.execute_reply":"2024-12-02T17:48:48.370662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tweet.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T17:48:57.705395Z","iopub.execute_input":"2024-12-02T17:48:57.705845Z","iopub.status.idle":"2024-12-02T17:48:57.727153Z","shell.execute_reply.started":"2024-12-02T17:48:57.705805Z","shell.execute_reply":"2024-12-02T17:48:57.725781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tweet.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:59:02.438736Z","iopub.execute_input":"2024-12-02T14:59:02.439152Z","iopub.status.idle":"2024-12-02T14:59:02.458512Z","shell.execute_reply.started":"2024-12-02T14:59:02.439121Z","shell.execute_reply":"2024-12-02T14:59:02.457318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tweet.drop_duplicates(subset=['text'], keep=False, inplace=True)\ntrain_tweet.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T17:49:42.331454Z","iopub.execute_input":"2024-12-02T17:49:42.331904Z","iopub.status.idle":"2024-12-02T17:49:42.997670Z","shell.execute_reply.started":"2024-12-02T17:49:42.331866Z","shell.execute_reply":"2024-12-02T17:49:42.996015Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TFIDF\n","metadata":{}},{"cell_type":"code","source":"y_train_data = train_tweet['emotion']\nX_train_data = train_tweet.drop(['tweet_id', 'emotion', 'identification', 'hashtags'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_train_data, y_train_data, test_size=0.2, random_state=42, stratify=y_train_data\n)\n\ntfidf = TfidfVectorizer(max_features=1500)\nX = tfidf.fit_transform(X_train['text']).toarray()\nX_test = tfidf.transform(X_test['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:27:30.174317Z","iopub.execute_input":"2024-12-02T18:27:30.174769Z","iopub.status.idle":"2024-12-02T18:27:49.479074Z","shell.execute_reply.started":"2024-12-02T18:27:30.174731Z","shell.execute_reply":"2024-12-02T18:27:49.477921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"le = LabelEncoder()\ny = le.fit_transform(y_train)\ny_test = le.transform(y_test)\n\nclf = RandomForestClassifier()\nclf.fit(X, y)\n\ny_pred = clf.predict(X_test)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n\n\nX_test_data = test_tweet.drop(['tweet_id', 'identification', 'hashtags'], axis=1)\nX_test_data = tfidf.transform(X_test_data['text']).toarray()\n\ny_test_pred = clf.predict(X_test_data)\n\ny_pred_labels = le.inverse_transform(y_test_pred)\n\nsubmission = pd.DataFrame({\n    'id': test_tweet['tweet_id'],\n    'emotion': y_pred_labels\n})\n\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:28:03.161072Z","iopub.execute_input":"2024-12-02T18:28:03.161489Z","iopub.status.idle":"2024-12-02T18:33:35.009427Z","shell.execute_reply.started":"2024-12-02T18:28:03.161443Z","shell.execute_reply":"2024-12-02T18:33:35.007797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_data = test_tweet.drop(['tweet_id', 'identification', 'hashtags'], axis=1)\nX_test_data = tfidf.transform(X_test_data['text']).toarray()\n\ny_test_pred = clf.predict(X_test_data)\n\ny_pred_labels = le.inverse_transform(y_test_pred)\n\nsubmission = pd.DataFrame({\n    'id': test_tweet['tweet_id'],\n    'emotion': y_pred_labels\n})\n\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T18:33:39.768995Z","iopub.execute_input":"2024-12-02T18:33:39.769421Z","iopub.status.idle":"2024-12-02T18:34:06.672655Z","shell.execute_reply.started":"2024-12-02T18:33:39.769382Z","shell.execute_reply":"2024-12-02T18:34:06.670960Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transformer","metadata":{}},{"cell_type":"markdown","source":"# Bert\n","metadata":{}}]}